# we can later decide if we want to include it as a feature
# Factor variables/ char types considered as nominal variables
nominal_vars <- c("Cat_A", "Cat_B", "Cat_C", "Cat_D", "Cat_E", "Cat_F", "Cat_G", "Cat_H",
"Cat_I", "Cat_J", "Cat_K", "Cat_L", "Cat_M", "Cat_N")
# we consider int type variables as ordinal
ordinal_vars <- c("Count_1", "Count_2", "Count_3")
# Contiuous is specified as continuous in instructions doc, we will also consider Days_from_mfg as a continuous variable
continuous_vars <- c("Continuous", "Days_from_mfg")
# temporary omition of missing data using na.rm for running univariate
# mean
train_data[, lapply(.SD, mean, na.rm=TRUE), .SDcols=c(continuous_vars, ordinal_vars, "Flag")]
# mean
train_data[, lapply(.SD, median, na.rm=TRUE), .SDcols=c(continuous_vars, ordinal_vars, "Flag")]
# range
train_data[, lapply(.SD, range, na.rm=TRUE), .SDcols=c(continuous_vars, ordinal_vars, "Flag")]
# std dev
train_data[, lapply(.SD, sd, na.rm=TRUE), .SDcols=c(continuous_vars, ordinal_vars, "Flag")]
# IQR
train_data[, lapply(.SD, IQR, na.rm=TRUE), .SDcols=c(continuous_vars, ordinal_vars, "Flag")]
hist(na.omit(train_data[,Days_from_mfg]), freq = F)
hist(na.omit(train_data[,Continuous], freq = F))
hist(na.omit(train_data[,Count_1], freq = F))
hist(na.omit(train_data[,Count_2], freq = F))
hist(na.omit(train_data[,Count_3], freq = F))
# none of the distributions are normal
# majority of them are exponential
# boxplot for these distributions will not show interpretable results
# many values will be above the 75th percentile so we may have many outliers in the data
qplot(data = train_data[,.(count=.N), by=c("Cat_A")], x=Cat_A, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_B")], x=Cat_B, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_C")], x=Cat_C, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_D")], x=Cat_D, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_E")], x=Cat_E, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_F")], x=Cat_F, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_G")], x=Cat_G, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_H")], x=Cat_H, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_I")], x=Cat_I, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_J")], x=Cat_J, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_K")], x=Cat_K, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_L")], x=Cat_L, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_M")], x=Cat_M, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_N")], x=Cat_N, y=count)
# Cat_G, Cat_H, Cat_I, Cat_J and Cat_L have multiple categories but very few categories with high count of observations
# Cat_A, Cat_D, Cat_E, Cat_F have large number of categories and the count of observations under them are almost evenly spread
continuous_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)),.(Continuous)][order(Continuous)]
qplot(data = continuous_var, x = Continuous, y = positive_response_rate, geom = "line") + geom_smooth(method = "lm")
# with increase in Continuous positive response rate appears to go down
Days_from_mfg_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)),.(Days_from_mfg)][order(Days_from_mfg)]
qplot(data = Days_from_mfg_var, x = Days_from_mfg, y = positive_response_rate, geom = "line") + geom_smooth(method = "lm")
# with increase in Days_from_mfg_var positive response rate appears to go up
Count_1_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)),.(Count_1)][order(Count_1)]
qplot(data = Count_1_var, x = Count_1, y = positive_response_rate, geom = "line") + geom_smooth(method = "lm")
# with increase in Count_1 positive response rate appears to  go down
Count_2_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)),.(Count_2)][order(Count_2)]
qplot(data = Count_2_var, x = Count_2, y = positive_response_rate, geom = "line") + geom_smooth(method = "lm")
# with increase in Count_2 positive response rate appears to  go up
Count_3_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)),.(Count_3)][order(Count_3)]
qplot(data = Count_3_var, x = Count_3, y = positive_response_rate, geom = "line") + geom_smooth(method = "lm")
# with increase in Count_2 positive response rate appears to gradually go up (this variable may not be of much importance while building a model)
Cat_A_var <- train_data[,.(positive_response_rate = factormean(Target), count = length(ID)), .(Cat_A)] [order(-positive_response_rate)]
qplot(data = Cat_A_var, x = positive_response_rate, main = "Histogram of positive response rate frequency")
# majority of categories under Cat_A show a response rate < 0.5
# we can also assign a low, medium / high bucket (create bins of categories which may help better our predictions)
Cat_B_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_B)] [order(-positive_response_rate)]
qplot(data = Cat_B_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_B", xlab = "positive response rate")
# All of categories under Cat_B show a response rate < 0.5, also this variable has a lot of blank values, it may not be considered while building the model
Cat_C_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_C)] [order(-positive_response_rate)]
qplot(data = Cat_C_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_C", xlab = "positive response rate")
# majority of categories under Cat_C show a response rate < 0.3
Cat_D_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_D)] [order(-positive_response_rate)]
qplot(data = Cat_D_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_D", xlab = "positive response rate")
# majority of categories under Cat_D show a response rate < 0.5
Cat_E_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_E)] [order(-positive_response_rate)]
qplot(data = Cat_E_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_E", xlab = "positive response rate")
# majority of categories under Cat_E show a response rate < 0.5
Cat_F_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_F)] [order(-positive_response_rate)]
qplot(data = Cat_F_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_F", xlab = "positive response rate")
# majority of categories under Cat_F show a response rate < 0.5
Cat_G_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_G)] [order(-positive_response_rate)]
qplot(data = Cat_G_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_G", xlab = "positive response rate")
# majority of categories under Cat_G show a response rate of 0
Cat_H_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_H)] [order(-positive_response_rate)]
qplot(data = Cat_H_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_H", xlab = "positive response rate")
# categories seem evenly distributed in terms of positive response, this variable may not explain much variance in response
Cat_I_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_I)] [order(-positive_response_rate)]
qplot(data = Cat_I_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_I", xlab = "positive response rate")
# majority of categories under Cat_I show a response rate < 0.5
Cat_J_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_J)] [order(-positive_response_rate)]
qplot(data = Cat_J_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_J", xlab = "positive response rate")
# majority of categories under Cat_J show a response rate < 0.4
Cat_K_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_K)] [order(-positive_response_rate)]
qplot(data = Cat_K_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_K", xlab = "positive response rate")
# majority of categories under Cat_K show an even distribution
Cat_L_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_L)] [order(-positive_response_rate)]
qplot(data = Cat_L_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_L", xlab = "positive response rate")
# majority of categories under Cat_L show a response rate < 0.5
Cat_M_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_M)] [order(-positive_response_rate)]
qplot(data = Cat_M_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_M", xlab = "positive response rate")
# only 2 categories and not much variation, this might not be of much importance
Cat_N_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_N)] [order(-positive_response_rate)]
qplot(data = Cat_N_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_N", xlab = "positive response rate")
# majority of categories under Cat_N show a response rate > 0.4
Flag_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Flag)] [order(-positive_response_rate)]
qplot(data = Flag_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Flag", xlab = "positive response rate")
# we do not observe much variation wrt Target in this variable, also majority of the observations are missing
# checking for significance wrt Target
contingency_table <- dcast(train_data, Target ~ Flag, value.var = 'Flag', fun = length)
chisq.test(contingency_table)
# This shows that proportions of 1's and 0's wrt to Target is statistically significant
ggcorr(train_data[,.SD,.SDcols= c(continuous_vars, ordinal_vars)])
# COunt_2 appears to be highly correlated with COunt_3, we will omit either 1 of these variables
# as we are not clear on the objective or business reasons we will remove Count_3
columns_to_remove = c("Count_3")
# obtain percent NULL values for every column
percent_nulls <- function(x){
(sum((is.na(x)) | (x=="") | (x=="NA") | (x=="na") | (sum(is.nan(x))) | sum(is.null(x)))/length(x) * 100)
}
sapply(train_data, percent_nulls)
# Cat_B, Cat_G, Flag, Count_2, Count_3 have ~67% missing/NULL values
# we will remove these columns as they have a lot of NA values, we cannot impute the high number of missing values with any technique
# as it might skew our data and introduce noise which could get modelled giving the wrong Beta estimates for corresponding columns
columns_to_remove = c(columns_to_remove, "Cat_B", "Cat_G", "Flag", "Count_2")
# removing the specific columns
train_data <- train_data[, c(columns_to_remove):=NULL]
sapply(train_data, percent_nulls)
train_data_wo_target <- train_data[,.SD,.SDcols = (colnames(train_data) %nin% c("Target", "ID"))]
# gc()
# rm(list = ls()[grep("Count", ls())])
# fittedmice <- mice(train_data_wo_target, meth='rf')
# saveRDS(fittedmice, file = "mice_output.rds")
fittedmice <- readRDS("mice_output.rds")
imputed_train_data <- complete(fittedmice)
imputed_train_data <- data.table(cbind(imputed_train_data, train_data[,.SD,.SDcols = (colnames(train_data) %in% c("Target", "ID"))]))
sapply(imputed_train_data, percent_nulls)
# All variables have been imputed
# obtaining values above and below 3*IQR
subset_iqr <- function(x) {
((x < ((quantile(x, c(0.25))) - (3 * (IQR(x))))) |
(x > ((quantile(x, c(0.75))) + (3 * (IQR(x))))))
}
# obtianing percent values above and below 3*IQR
percent_above_and_below_iqr <- function(x){
(length(which(
subset_iqr(x)
))/length(x)) * 100
}
# assining NAs to subsetted values
assign_na <- function(x){
ifelse(subset_iqr(x) == TRUE, NA, x)
}
# continuous_vars
# percent observations above and below 3*IQR
imputed_train_data[,lapply(.SD, percent_above_and_below_iqr), .SDcols = c(continuous_vars, "Count_1")]
# there are ~4% values which lie outside 3*IQR range for Continuous variable, for Days_from_mfg and Count_1
# this value is zero, we can consider these as outliers and replace them with NA's, we will use MICE to impute
# them again
# assing NA values to outliers
for (i in c("Continuous")){
imputed_train_data <- imputed_train_data[,(i):= lapply(.SD, assign_na), .SDcols = c(i)]
}
# NAs intrduced in data
sapply(imputed_train_data, percent_nulls)
imputed_train_data_wo_target <- imputed_train_data[,.SD,.SDcols = (colnames(imputed_train_data) %nin% c("Target", "ID"))]
# gc()
#
# fittedmice_outlier <- mice(imputed_train_data_wo_target, meth='rf')
# saveRDS(fittedmice_outlier, file = "mice_outlier_output.rds")
fittedmice_outlier <- readRDS("mice_outlier_output.rds")
outlier_treated_train_data <- data.table(complete(fittedmice_outlier))
outlier_treated_train_data <- data.table(cbind(outlier_treated_train_data, imputed_train_data[,.SD,.SDcols = c("Target", "ID")]))
sapply(outlier_treated_train_data, percent_nulls)
mdistance <- mahalanobis(outlier_treated_train_data[,.SD,.SDcols=c(continuous_vars, "Count_1")],
center = colMeans(outlier_treated_train_data[,.SD,.SDcols=c(continuous_vars, "Count_1")]),
cov = cov(outlier_treated_train_data[,.SD,.SDcols=c(continuous_vars, "Count_1")]))
outlier_treated_train_data <- outlier_treated_train_data[,mdistance := mdistance]
outlier_treated_train_data <- outlier_treated_train_data[,moutlier := ifelse(mdistance > 30, 1, 0)]
nrow(outlier_treated_train_data[moutlier == 1,])
# 6 rows are tagged as outliers, as these are very low in number we can remove them from the dataset
multivariate_outlier_treated_train_data <- outlier_treated_train_data[moutlier == 0,]
multivariate_outlier_treated_train_data <- multivariate_outlier_treated_train_data[,':='(mdistance= NULL, moutlier=NULL)]
sapply(multivariate_outlier_treated_train_data, percent_nulls)
str(multivariate_outlier_treated_train_data)
saveRDS(multivariate_outlier_treated_train_data, "pre_processed_train_data.rds")
getwd()
knitr::opts_chunk$set(echo = TRUE)
# Modifying test data to make predictions
# we remove Cat_M variable as there are new levels which have been introduced in the test set which
# are not present train data
test_mat <- sparse.model.matrix(~.+0,data = test_data[, !names(test_data) %in% c("Target", "Cat_M", "ID")])
knitr::opts_chunk$set(echo = TRUE)
rm(list=ls())
library(mice)
library(data.table)
library(ggplot2)
library(caret)
library(rBayesianOptimization)
library(xgboost)
library(Matrix)
library(ROCR)
library(InformationValue)
library(caret)
library(ROSE)
library(pscl)
`%nin%` <- Negate(`%in%`)
train_data <- readRDS("pre_processed_train_data.rds")
train_data <- data.table(train_data)
train_data
test_data <- read.csv("../Ineight-Analyst-Testing-Data.csv")
test_data <- data.table(test_data)
test_data
# creating Day_from_mfg variable from Date column
test_data <- test_data[,Date:= as.Date(Date)]
test_data <- test_data[,Days_from_mfg:= as.numeric(Sys.Date() - Date)]
test_data <- test_data[,Date:= NULL]
# subsetting columns of test data so that they contain the same columns as train data
cols_to_omit <- colnames(test_data)[which(colnames(test_data) %nin% colnames(train_data))]
test_data <- test_data[,(cols_to_omit) := NULL]
colnames(test_data)
# Factor variables/ char types considered as ordinal variables
nominal_vars <- c("Cat_A", "Cat_C", "Cat_D", "Cat_E", "Cat_F", "Cat_H",
"Cat_I", "Cat_J", "Cat_K", "Cat_L", "Cat_M", "Cat_N")
# int type variables are considered nominal
ordinal_vars <- c("Count_1")
# specified as continuous in instructions doc
continuous_vars <- c("Continuous", "Days_from_mfg")
# obtain percent NULL values for every column
percent_nulls <- function(x){
(sum((is.na(x)) | (x=="") | (x=="NA") | (x=="na") | (sum(is.nan(x))) | sum(is.null(x)))/length(x) * 100)
}
sapply(test_data, percent_nulls)
# continuous and ordinal variables do not have missing values
# low percent of missing values is observed for Nominal variables
# assigning "NA" to Nominal values with missing data
nominal_var_impute <- test_data[, lapply(.SD, as.character), .SDcols=c(nominal_vars)]
test_data <- test_data[,(nominal_vars) := NULL]
nominal_var_impute[(nominal_var_impute=="") | (nominal_var_impute=="na") | (is.na(nominal_var_impute))] <- NA
nominal_var_impute <- nominal_var_impute[, lapply(.SD, as.factor)]
# bind continuous and nominal vars together as a data table
test_data_w_na <- data.table(cbind(test_data, nominal_var_impute))
test_data_wo_id <- test_data_w_na[,.SD,.SDcols = (colnames(test_data_w_na) %nin% c("ID"))]
# fitted_mice_test <- mice(test_data_wo_id, meth='rf')
# saveRDS(fitted_mice_test, "mice_test_data.RDS")
fitted_mice_test <- readRDS("mice_test_data.RDS")
# imputation step
imputed_test_data <- data.table(complete(fitted_mice_test))
sapply(imputed_test_data, percent_nulls)
# check if any other variable has missing data
test_data <- copy(imputed_test_data)
# univariate outlier detection and treatment
# transforming test data based on train data range
train_data[, lapply(.SD, range), .SDcols = c(continuous_vars, ordinal_vars)]
imputed_test_data <- imputed_test_data[(Continuous <= 0 | Continuous >= 40226437), Continuous:= NA]
imputed_test_data <- imputed_test_data[(Days_from_mfg <= 952 | Days_from_mfg >= 8270), Days_from_mfg:= NA]
imputed_test_data <- imputed_test_data[(Count_1 <= 1 | Count_1 >= 13), Continuous:= NA]
sapply(imputed_test_data, percent_nulls)
# we can observe that Days_from_mfg shows almost everything as outliers - this can be because of time periods not
# overlapping when between train and test, we will omit the outlier treatment for this variable
# we will skip using this variable in model fit and remove it from train and test data
imputed_test_data <- imputed_test_data[,Days_from_mfg := NULL]
test_data <- test_data[,Days_from_mfg := NULL]
train_data <- train_data[,Days_from_mfg := NULL]
continuous_vars <- c("Continuous")
# wrt Continuous we have 33% observations being tagged as outliers, it is a sizeable number
# As this is the test data and the objective of building our classification model is not clear
# (eg. for fraud we might have to predict such outlier cases)
# we will leave this variables untreated
# multivariate outlier detection and treatment
mdistance <- mahalanobis(test_data[,.SD,.SDcols=c(continuous_vars, ordinal_vars)],
center = colMeans(test_data[,.SD,.SDcols=c(continuous_vars, ordinal_vars)]),
cov = cov(test_data[,.SD,.SDcols=c(continuous_vars, ordinal_vars)]))
test_data <- test_data[,mdistance := mdistance]
test_data <- test_data[,moutlier := ifelse(mdistance > 30, 1, 0)]
nrow(test_data[moutlier == 1,])
# 10 observations are tagged as outliers
# as we need predictions for all 837 observations it will not make sense to remove these rows, we will skip removing these
# removing moutlier and mdistance column
test_data <- test_data[,":=" (moutlier = NULL, mdistance = NULL)]
test_data <- as.data.frame(test_data)
# atm we dont have Target variable for our test data
# so we wouldnt be able to check the performance of our classification models
# to overcome this we create a stratified train and test sample from the train data itself, we call this
# validation data to avoid confusion with the original test dataset
train_data <- as.data.frame(train_data)
set.seed(123) # reproducible results
index <- createDataPartition(train_data$Target, p = .70, list = FALSE)
train_data_sample <- train_data[index, ]
validation_data_sample <- train_data[-index, ]
nrow(train_data_sample)
nrow(validation_data_sample)
# this part is commented for knitting the rmd
# logit_model <- glm(Target ~ .-ID, data = train_data_sample, family = binomial)
# model failed to converge because of singularities
# summary(logit_model)
# we encounter singularities for Cat_D, Cat_E and Cat_N
# For Cat_F we observe that the split between categories is not reflected uniformly in validation set
# we will remove these columns and try fitting the model again
cols_to_subset <- colnames(validation_data_sample)[colnames(validation_data_sample) %nin% c("ID", "Cat_D", "Cat_E", "Cat_F", "Cat_N")]
train_data_sample_wo_singularity <- train_data_sample[,cols_to_subset]
logit_model_wo_singularities <- glm(Target ~ ., data = train_data_sample_wo_singularity, family = binomial)
# summary(logit_model_wo_singularities)
# model fit is a success
pR2(logit_model_wo_singularities)
# mcFaddens r^2 has a very low value (0.19) this means that the model performance is bad
# remove columns that caused singularities while fitting, from the test set
validation_data_sample_singularity <- validation_data_sample[,cols_to_subset]
# get predictions
logit_pred <- predict(logit_model_wo_singularities, validation_data_sample_singularity, type = 'response')
# plot ROC curve
ROCRpred <- prediction(logit_pred, validation_data_sample_singularity$Target)
ROCRperf <- performance(ROCRpred, 'tpr','fpr')
plot(ROCRperf)
# Calculate AUROC
auc <- performance(ROCRpred, measure = "auc")
auc <- auc@y.values[[1]]
auc
# obtain optimal cutoff value, the default for this is to optimize for both 1s and 0s
# as we do not know the business objective we keep this as default which is both 1s and 0s
cutoff <- optimalCutoff(validation_data_sample$Target, logit_pred)[1]
cutoff
# convert probabilty scores to classes
logitpred <- ifelse(logit_pred > cutoff, 1, 0)
caret::confusionMatrix(as.factor(validation_data_sample$Target), as.factor(logitpred))
# Confusion matrix shows a poor performing classifier
# we might be able to improve it if we use a non paramteric classification techniques
train_features <- train_data_sample[,c(continuous_vars, ordinal_vars, nominal_vars)]
train_target <- train_data_sample[,c("Target")]
validation_features <- validation_data_sample[,c(continuous_vars, ordinal_vars, nominal_vars)]
validation_target <- validation_data_sample[,c("Target")]
# since XGB splits trees based on ordered list we will have to one hot encode the nominal variables
# continuous and ordinal variables remain as is
encoded_train_features <- sparse.model.matrix(~.+0,data = train_features[, !names(train_features) %in% "Target"])
encoded_validation_features <- sparse.model.matrix(~.+0,data = validation_features[, !names(validation_features) %in% "Target"])
# Since we have only 1 Continuous variable we will not use scaling on the dataset
# we require train data to be in the form of a matrix before running xgb
dtrain <- xgb.DMatrix(encoded_train_features, label = train_target)
# same for validation
dtest <- xgb.DMatrix(data = encoded_validation_features,label = validation_target)
# specifying 5-fold cross validation(decreases variance/overfit)
cv_folds <- KFold(train_target, nfolds = 5,stratified = TRUE, seed = 0)
# initialise cross validation
xgb_cv_bayes <- function(max_depth, min_child_weight, subsample, eta, colsample_bytree, nrounds, gamma) {
cv <- xgb.cv(params = list(booster = "gbtree",
eta = eta,
max_depth = max_depth,
min_child_weight = min_child_weight,
subsample = subsample, # number of rows given to tree
colsample_bytree = colsample_bytree, # number of columns given to tree
gamma = gamma, # to induce regularization and prevent overfit
objective = "binary:logistic",
eval_metric = "auc"),
data = dtrain,
nrounds = nrounds,
folds = cv_folds,
prediction = TRUE,
showsd = TRUE,
early_stopping_rounds = 100,
maximize = TRUE,
verbose = 0)
list(Score = cv$evaluation_log$test_auc_mean[cv$best_iteration],
Pred = cv$pred)
}
# for gamma tuning, max_depth, and min_child_weight refer:
# https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6
# we will use bayesian optimization for tuning the paramters
# it tries to build a proxy approximation for the objective function and iterate over a grid of paramters to find the minima
OPT_Res <- BayesianOptimization(xgb_cv_bayes,bounds = list(max_depth = c(4L, 6L),
min_child_weight = c(0.1, 1L),
subsample = c(0.1, 0.6),
eta = c(0.001,0.4), # learning rate
colsample_bytree = c(0.6, 1L), # subset of columns
nrounds = c(100L, 300L), # number of estimators
gamma = c(3, 10)    # decrease overfit
),
init_grid_dt = NULL, init_points = 5, n_iter = 10,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
# split to params list to give as an input to
params_list <- lapply(split(OPT_Res$Best_Par, names(OPT_Res$Best_Par)), unname)
params_list
# fit using params taken from cv
xgb_fit <- xgb.train (params = params_list,
data = dtrain,
nrounds = params_list$nrounds,
print_every_n = 10,
early_stopping_rounds = 100,
maximize = TRUE ,
objective = "binary:logistic",
watchlist = list(val=dtest,train=dtrain),
eval_metric = "auc") # for classification
# get probabilty scores for validation set
xgb_pred <- predict(xgb_fit, dtest)
# plot ROC
ROCRpred <- prediction(xgb_pred, validation_target)
ROCRperf <- performance(ROCRpred, 'tpr','fpr')
plot(ROCRperf)
# get AUROC
auc <- performance(ROCRpred, measure = "auc")
auc <- auc@y.values[[1]]
auc
# obtain optimal cutoff
cutoff <- optimalCutoff(validation_target, xgb_pred)[1]
cutoff
# get confussion matric
xgbpred <- ifelse (xgb_pred > cutoff, 1, 0)
caret::confusionMatrix(as.factor(validation_target), as.factor(xgbpred))
table(train_data$Target)
# the dataset appears to be imbalanced, to balance out the classes we can either use over sampling or
# undersampling or else penalise the class which has more observations by using scale_pos_weight (python specific xgboost)
# (ratio of negative class to positive class)
weight_0 = 5638/10329
weight_0
# oversampling can be done using SMOTE where we intrduce synthetic values for independent variables for the dependent class that is low in numbers
# induce oversampling
train_smote <- ROSE(Target ~ ., data = train_data, seed = 1)$data
# check class balance for Target
table(train_smote$Target)
# create stratified sample for train and validation
set.seed(123) # reproducible results
index <- createDataPartition(train_smote$Target, p = .70, list = FALSE)
train_data_smote_sample <- train_smote[index, ]
validation_data_smote_sample <- train_smote[-index,]
train_features <- train_data_smote_sample[,c(continuous_vars, ordinal_vars, nominal_vars)]
train_target <- train_data_smote_sample[,c("Target")]
validation_features <- validation_data_smote_sample[,c(continuous_vars, ordinal_vars, nominal_vars)]
validation_target <- validation_data_smote_sample[,c("Target")]
# since XGB splits trees based on ordered list we will have to one hot encode the ordinal and nominal variables
# continuous variables remain as is
encoded_train_features <- sparse.model.matrix(~.+0,data = train_features[, !names(train_features) %in% "Target"])
encoded_validation_features <- sparse.model.matrix(~.+0,data = validation_features[, !names(validation_features) %in% "Target"])
# we require train data to be in the form of a matrix before running xgb
dtrain <- xgb.DMatrix(encoded_train_features, label = train_target)
# same for test
dtest <- xgb.DMatrix(data = encoded_validation_features,label = validation_target)
# specifying 5-fold cross validation(decreases variance/overfit)
cv_folds <- KFold(train_target, nfolds = 5,stratified = TRUE, seed = 0)
# initialise cross validation
xgb_cv_bayes <- function(max_depth, min_child_weight, subsample, eta, colsample_bytree, nrounds, gamma) {
cv <- xgb.cv(params = list(booster = "gbtree",
eta = eta,
max_depth = max_depth,
min_child_weight = min_child_weight,
subsample = subsample, # number of rows given to tree
colsample_bytree = colsample_bytree, # number of columns given to tree
gamma = gamma,
objective = "binary:logistic",
eval_metric = "auc"),
data = dtrain,
nrounds = nrounds,
folds = cv_folds,
prediction = TRUE,
showsd = TRUE,
early_stopping_rounds = 100,
maximize = TRUE,
verbose = 0)
list(Score = cv$evaluation_log$test_auc_mean[cv$best_iteration],
Pred = cv$pred)
}
OPT_Res_smote <- BayesianOptimization(xgb_cv_bayes, bounds = list(max_depth = c(4L, 6L),
min_child_weight = c(0.1, 1L),
subsample = c(0.1, 0.6),
eta = c(0.001,0.4), # learning rate
colsample_bytree = c(0.6, 1L), # subset of columns
nrounds = c(100L, 300L), # number of estimators
gamma = c(3, 10)    # decrease overfit
),
init_grid_dt = NULL, init_points = 5, n_iter = 10,
acq = "ucb", kappa = 2.576, eps = 0.0,
verbose = TRUE)
params_list <- lapply(split(OPT_Res_smote$Best_Par, names(OPT_Res_smote$Best_Par)), unname)
params_list
xgb_smote <- xgboost (params = params_list,
data = dtrain,
nrounds = params_list$nrounds, # from params_list
print_every_n = 10,
early_stopping_rounds = 100,
maximize = TRUE ,
objective = "binary:logistic", # for classification
eval_metric = "auc")
xgbpred_smote <- predict(xgb_smote, dtest)
ROCRpred <- prediction(xgbpred_smote, validation_target)
ROCRperf <- performance(ROCRpred, 'tpr','fpr')
plot(ROCRperf)
auc <- performance(ROCRpred, measure = "auc")
auc <- auc@y.values[[1]]
auc
# we observe a slight increase in AUC
# we will use this as the final model
cutoff <- optimalCutoff(validation_target, xgbpred_smote)[1]
cutoff
xgbpred_smote <- ifelse(xgbpred_smote > cutoff, 1, 0)
caret::confusionMatrix(as.factor(as.numeric(validation_target)), as.factor(xgbpred_smote))
# Modifying test data to make predictions
# we remove Cat_M variable as there are new levels which have been introduced in the test set which
# are not present train data
test_mat <- sparse.model.matrix(~.+0,data = test_data[, !names(test_data) %in% c("Target", "Cat_M", "ID")])
test_dmatrix <- xgb.DMatrix(data = test_mat)
# Predicting for test data
test_xgb_smote_pred <- predict(xgb_smote, test_dmatrix)
# writing probablity scores to csv
test_dat_ID <- read.csv("../Ineight-Analyst-Testing-Data.csv")[,c("ID")]
subission_df <- cbind(as.character(test_dat_ID), test_xgb_smote_pred)
colnames(subission_df) <- c("ID", "Target")
write.csv("ineight_analyst_challenge_predictions_prathmesh.csv", row.names = F)
getwd()
write.csv("ineight_analyst_challenge_predictions_prathmesh.csv", row.names = F)
write.csv("11ineight_analyst_challenge_predictions_prathmesh.csv", row.names = F)
write.csv("11ineight_analyst_challenge_predictions_prathmesh.csv", row.names = F)
getwd
getwd()
write.csv("11ineight_analyst_challenge_predictions_prathmesh.csv", row.names = F)
write.csv("ineight_analyst_challenge_predictions_prathmesh.csv", row.names = F)
write.csv(subission_df, "ineight_analyst_challenge_predictions_prathmesh.csv", row.names = F)
