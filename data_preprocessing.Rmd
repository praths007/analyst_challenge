---
title: "ineight_data_preprocessing"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Initialization

```{r Loading libraries and functions, echo=F, results= F}
rm(list=ls())
library(mice)
library(dplyr)
library(data.table)
library(ggplot2)
library(GGally)
factormean <- function(x) {sum(x == 1)/length(x)}

`%nin%` <- Negate(`%in%`)

```



```{r}
train_data <- read.csv("../Ineight-Analyst-Training-Data.csv")
train_data <- data.table(train_data)
train_data
```

```{r, datatypes of variables}

str(train_data)

```

```{r,  handling date column, collapse=TRUE}
# checking relation betw date column and target column

print(nrow(unique(train_data[,Date])))

print(nrow(unique(train_data[,ID])))

# there isnt a 1 to 1 mapping between ID's and Date, to utilize the date column
# we can consider it to be manufacture date of a particular ID
# we could derive a continuous column called Days_from_manufacture

train_data[,Date:= as.Date(Date)]
train_data[,Days_from_mfg:= as.numeric(Sys.Date() - Date)]
train_data[,Date:= NULL]
str(train_data)
```
```{r, checking NULL values}

train_data

```



```{r, investigating flag column}

train_data[,.(count = .N), by=Flag]

# this variable has a lot of missing values, for the time being we consider it as a separete variable and
# we can later decide if we want to include it as a feature

```


```{r, variable splits}

# Factor variables/ char types considered as ordinal variables
nominal_vars <- c("Cat_A", "Cat_B", "Cat_C", "Cat_D", "Cat_E", "Cat_F", "Cat_G", "Cat_H",
                 "Cat_I", "Cat_J", "Cat_K", "Cat_L", "Cat_M", "Cat_N")

# int type variables are considered nominal
ordinal_vars <- c("Count_1", "Count_2", "Count_3")

# specified as continuous in instructions doc
continuous_vars <- c("Continuous", "Days_from_mfg")

```

# Univariate Analysis

## Continuous and ordinal variable checks

```{r, Univariate analysis - continuous variables, collapse=TRUE}

# temporary omition of missing data using na.rm for running univariate

print("mean")
print(train_data[, lapply(.SD, mean, na.rm=TRUE), .SDcols=c(continuous_vars, ordinal_vars, "Flag")])

print("median")
print(train_data[, lapply(.SD, median, na.rm=TRUE), .SDcols=c(continuous_vars, ordinal_vars, "Flag")])

print("range")
print(train_data[, lapply(.SD, range, na.rm=TRUE), .SDcols=c(continuous_vars, ordinal_vars, "Flag")])

print("std_dev")
print(train_data[, lapply(.SD, sd, na.rm=TRUE), .SDcols=c(continuous_vars, ordinal_vars, "Flag")])

print("IQR")
print(train_data[, lapply(.SD, IQR, na.rm=TRUE), .SDcols=c(continuous_vars, ordinal_vars, "Flag")])

```


```{r, distribution plots - continuous variables}

hist(na.omit(train_data[,Days_from_mfg]), freq = F)

hist(na.omit(train_data[,Continuous], freq = F))

hist(na.omit(train_data[,Count_1], freq = F))

hist(na.omit(train_data[,Count_2], freq = F))

hist(na.omit(train_data[,Count_3], freq = F))

# none of the distributions are normal
# majority of them are exponential
# boxplot for these distributions will not show interpretable results
# many values will be above the 75th percentile so we may have many outliers in the data


```



## Nominal Variables


```{r, distribution of nominal variables}

qplot(data = train_data[,.(count=.N), by=c("Cat_A")], x=Cat_A, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_B")], x=Cat_B, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_C")], x=Cat_C, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_D")], x=Cat_D, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_E")], x=Cat_E, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_F")], x=Cat_F, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_G")], x=Cat_G, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_H")], x=Cat_H, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_I")], x=Cat_I, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_J")], x=Cat_J, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_K")], x=Cat_K, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_L")], x=Cat_L, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_M")], x=Cat_M, y=count)
qplot(data = train_data[,.(count=.N), by=c("Cat_N")], x=Cat_N, y=count)

# Cat_G, Cat_H, Cat_I, Cat_J and Cat_L have multiple categories but very few categories with high count of observations
# Cat_A, Cat_D, Cat_E, Cat_F have large number of categories and the count of observations under them are almost evenly spread

```


# Bivariate Analysis 

## Target - catagorical vs. Continuous + ordinal
```{r, bivariate - Continuous}

continuous_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)),.(Continuous)][order(Continuous)]

qplot(data = continuous_var, x = Continuous, y = positive_response_rate, geom = "line") + geom_smooth(method = "lm")

# with increase in Continuous positive response rate appears to go down

```




```{r, bivariate - Days_from_mfg}
Days_from_mfg_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)),.(Days_from_mfg)][order(Days_from_mfg)]

qplot(data = Days_from_mfg_var, x = Days_from_mfg, y = positive_response_rate, geom = "line") + geom_smooth(method = "lm")

# with increase in Days_from_mfg_var positive response rate appears to go up

```


```{r, Count_1}

Count_1_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)),.(Count_1)][order(Count_1)]

qplot(data = Count_1_var, x = Count_1, y = positive_response_rate, geom = "line") + geom_smooth(method = "lm")

# with increase in Count_1 positive response rate appears to  go down

```




```{r, Count 2}


Count_2_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)),.(Count_2)][order(Count_2)]

qplot(data = Count_2_var, x = Count_2, y = positive_response_rate, geom = "line") + geom_smooth(method = "lm")

# with increase in Count_2 positive response rate appears to  go up

```



```{r, Count 3}

Count_3_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)),.(Count_3)][order(Count_3)]

qplot(data = Count_3_var, x = Count_3, y = positive_response_rate, geom = "line") + geom_smooth(method = "lm")

# with increase in Count_2 positive response rate appears to gradually go up (this variable may not be of much importance while building a model)
```

## Target - catagorical vs. nominal


```{r, Cat_A}

Cat_A_var <- train_data[,.(positive_response_rate = factormean(Target), count = length(ID)), .(Cat_A)] [order(-positive_response_rate)]

Cat_A_var

qplot(data = Cat_A_var, x = positive_response_rate, main = "Histogram of positive response rate frequency")

# majority of categories under Cat_A show a response rate < 0.5

# we can also assign a low, medium / high bucket (create bins of categories which may help better our predictions)

```

```{r, Cat_B}

Cat_B_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_B)] [order(-positive_response_rate)]

Cat_B_var

qplot(data = Cat_B_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_B", xlab = "positive response rate")

# All of categories under Cat_B show a response rate < 0.5, also this variable has a lot of blank values, it may not be considered while building the model

```


```{r, Cat_C}

Cat_C_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_C)] [order(-positive_response_rate)]

Cat_C_var

qplot(data = Cat_C_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_C", xlab = "positive response rate")

# majority of categories under Cat_C show a response rate < 0.3

```
```{r, Cat_D}

Cat_D_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_D)] [order(-positive_response_rate)]

Cat_D_var

qplot(data = Cat_D_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_D", xlab = "positive response rate")

# majority of categories under Cat_D show a response rate < 0.5

```








```{r, Cat_E}

Cat_E_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_E)] [order(-positive_response_rate)]

Cat_E_var

qplot(data = Cat_E_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_E", xlab = "positive response rate")

# majority of categories under Cat_E show a response rate < 0.5

```






```{r, Cat_F}

Cat_F_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_F)] [order(-positive_response_rate)]

Cat_F_var

qplot(data = Cat_F_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_F", xlab = "positive response rate")

# majority of categories under Cat_F show a response rate < 0.5

```



```{r, Cat_G}

Cat_G_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_G)] [order(-positive_response_rate)]

Cat_D_var

qplot(data = Cat_G_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_G", xlab = "positive response rate")

# majority of categories under Cat_G show a response rate of 0

```



```{r, Cat_H}

Cat_H_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_H)] [order(-positive_response_rate)]

Cat_H_var

qplot(data = Cat_H_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_H", xlab = "positive response rate")

# categories seem evenly distributed in terms of positive response, this variable may not explain much variance in response

```



```{r, Cat_I}

Cat_I_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_I)] [order(-positive_response_rate)]

Cat_I_var

qplot(data = Cat_I_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_I", xlab = "positive response rate")

# majority of categories under Cat_I show a response rate < 0.5

```




```{r, Cat_J}

Cat_J_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_J)] [order(-positive_response_rate)]

Cat_J_var

qplot(data = Cat_J_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_J", xlab = "positive response rate")

# majority of categories under Cat_J show a response rate < 0.4

```



```{r, Cat_K}

Cat_K_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_K)] [order(-positive_response_rate)]

Cat_K_var

qplot(data = Cat_K_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_K", xlab = "positive response rate")

# majority of categories under Cat_K show an even distribution

```



```{r, Cat_L}

Cat_L_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_L)] [order(-positive_response_rate)]

Cat_L_var

qplot(data = Cat_L_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_L", xlab = "positive response rate")

# majority of categories under Cat_L show a response rate < 0.5

```



```{r, Cat_M}

Cat_M_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_M)] [order(-positive_response_rate)]

Cat_M_var

qplot(data = Cat_M_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_M", xlab = "positive response rate")

# only 2 categories and not much variation, this might not be of much importance

```



```{r, Cat_N}

Cat_N_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Cat_N)] [order(-positive_response_rate)]

Cat_N_var

qplot(data = Cat_N_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Cat_N", xlab = "positive response rate")

# majority of categories under Cat_N show a response rate > 0.4

```


```{r, Flag variable}


Flag_var <- train_data[,.(positive_response_rate = factormean(Target), Count = length(ID)), .(Flag)] [order(-positive_response_rate)]

Flag_var

qplot(data = Flag_var, x = positive_response_rate, main = "Histogram of positive response rate frequency",ylab = "Count of Flag", xlab = "positive response rate")

# we do not observe much variation wrt Target in this variable


contingency_table <- dcast(train_data, Target ~ Flag, value.var = 'Flag', fun = length)
chisq.test(contingency_table)

# This shows that proportions of 1's and 0's wrt to Target is statistically significant
```




## Correlation Analysis
```{r, correlation analysis}

ggcorr(train_data[,.SD,.SDcols= c(continuous_vars, ordinal_vars)])
# COunt_2 appears to be highly correlated with COunt_3, we will omit either 1 of these variables

columns_to_remove = c("Count_3")
```

## Missing value treatment
```{r, missing value treatment}

# obtain percent NULL values for every column
percent_nulls <- function(x){
  (sum((is.na(x)) | (x=="") | (x=="NA") | (x=="na") | (sum(is.nan(x))) | sum(is.null(x)))/length(x) * 100)
}

sapply(train_data, percent_nulls)

# Cat_B, Cat_G, Flag, Count_2, Count_3 have ~67% missing/NULL values
# we will remove these columns as they have a lot of NA values, we cannot impute the high number of missing values with any technique
# as it might skew our data and introduce noise which could get modelled giving the wrong Beta estimates for corresponding columns

columns_to_remove = c(columns_to_remove, "Cat_B", "Cat_G", "Flag", "Count_2")

columns_to_remove

# removing the specific columns
train_data[, c(columns_to_remove):=NULL]


sapply(train_data, percent_nulls)

```


```{r, Imputation of continuous + ordinal variables}

train_data_wo_target <- train_data[,.SD,.SDcols = (colnames(train_data) %nin% c("Target", "ID"))]

# gc()
# rm(list = ls()[grep("Count", ls())]) 

# fittedmice <- mice(train_data_wo_target, meth='rf')
# saveRDS(fittedmice, file = "mice_output.rds")

fittedmice <- readRDS("mice_output.rds")

imputed_train_data <- complete(fittedmice)

continuous_imputed_train_data <- data.table(cbind(imputed_train_data, train_data[,.SD,.SDcols = c("Target", "ID")]))

sapply(continuous_imputed_train_data, percent_nulls)
# Majority of continuous variables have been imputed
# Now the remaining Nominal variables will be assigned "Others", wherever we have missing data

```

```{r,  imputation of Nominal variables}

# temp changing factors to characters for substituting NAs with Others
nominal_vars_to_subset <- nominal_vars[which(nominal_vars %nin% columns_to_remove)]

nominal_var_impute <- continuous_imputed_train_data[, lapply(.SD, as.character), .SDcols=c(nominal_vars_to_subset)]
continuous_imputed_train_data[,(nominal_vars_to_subset) := NULL]

nominal_var_impute[(nominal_var_impute=="") | (nominal_var_impute=="NA") | (nominal_var_impute=="na")] <- "Others"
nominal_var_impute <- nominal_var_impute[,lapply(.SD, as.factor)]

# bind continuous and nominal vars together as a data table
imputed_train_data <- data.table(cbind(continuous_imputed_train_data, nominal_var_impute))

# checking if there are any other NULLS
sapply(imputed_train_data, percent_nulls)
# All missing valus have been imputed - 
# Continuous and ordinal features are imputed using MICE
# As other nominal features have very less percentage of missing values, we replace these missing values with "Other"
str(imputed_train_data)


```

## Outlier analysis for continuous and ordinal variables
### Univariate analysis for Outlier detection and treatment

```{r, continuous_vars and ordinal_vars outliers, collapse=TRUE}

# obtaining values above and below 3*IQR
subset_iqr <- function(x) {
     ((x < ((quantile(x, c(0.25))) - (3 * (IQR(x))))) |
                    
  (x > ((quantile(x, c(0.75))) + (3 * (IQR(x))))))
  
}

# obtianing percent values above and below 3*IQR
percent_above_and_below_iqr <- function(x){
(length(which(
 
subset_iqr(x)

  ))/length(x)) * 100            
                
}

# assining NAs to subsetted values
assign_na <- function(x){
ifelse(subset_iqr(x) == TRUE, NA, x)
}

# continuous_vars
print("percent observations above and below 3*IQR")
imputed_train_data[,lapply(.SD, percent_above_and_below_iqr), .SDcols = c(continuous_vars, "Count_1")]


# there are ~4% values which lie outside 3*IQR range for Continuous variable, for Days_from_mfg and Count_1
# this value is zero, we can consider these as outliers and replace them with NA's, we will use MICE to impute
# them again


# assing NA values to outliers
for (i in c("Continuous")){
imputed_train_data[,(i):= lapply(.SD, assign_na), .SDcols = c(i)]
}

# NAs intrduced in data
sapply(imputed_train_data, percent_nulls)

imputed_train_data_wo_target <- imputed_train_data[,.SD,.SDcols = (colnames(imputed_train_data) %nin% c("Target", "ID"))]

# gc()
# 
# fittedmice_outlier <- mice(imputed_train_data_wo_target, meth='rf')
# saveRDS(fittedmice_outlier, file = "mice_outlier_output.rds")

fittedmice_outlier <- readRDS("mice_outlier_output.rds")
outlier_treated_train_data <- data.table(complete(fittedmice_outlier))

outlier_treated_train_data <- data.table(cbind(outlier_treated_train_data, imputed_train_data[,.SD,.SDcols = c("Target", "ID")]))


sapply(outlier_treated_train_data, percent_nulls)
```




### Multivariate analysis for Outlier detection and treatment

```{r, multivariate outlier detection using mahalonobis distance}

mdistance <- mahalanobis(outlier_treated_train_data[,.SD,.SDcols=c(continuous_vars, "Count_1")], 
                      center = colMeans(outlier_treated_train_data[,.SD,.SDcols=c(continuous_vars, "Count_1")]),
                      cov = cov(outlier_treated_train_data[,.SD,.SDcols=c(continuous_vars, "Count_1")]))

outlier_treated_train_data[,mdistance := mdistance]

outlier_treated_train_data[,moutlier := ifelse(mdistance > 30, 1, 0)]
nrow(outlier_treated_train_data[moutlier == 1,])
# around 6 rows are tagged as outliers, as these are very low in number we can remove them from the dataset

multivariate_outlier_treated_train_data <- outlier_treated_train_data[moutlier == 0,]

multivariate_outlier_treated_train_data[,':='(mdistance= NULL, moutlier=NULL)]

sapply(multivariate_outlier_treated_train_data, percent_nulls)

str(multivariate_outlier_treated_train_data)

saveRDS(multivariate_outlier_treated_train_data, "pre_processed_train_data.RDS")

```

```{r}

```


